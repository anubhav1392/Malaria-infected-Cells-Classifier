{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Malaria Cells Classifier\n#Custom Model\n#No data augmentation done here except image reshaping since images were of varying shapes\n#Work in Progress","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nimport seaborn as sns\nfrom glob import glob\nfrom tqdm import tqdm\nimport numpy as np\nfrom skimage import io\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Path\nimg_path='../input/cell_images/cell_images'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target=[]\nimages=[]\nfor class_ in os.listdir(img_path):\n    files=glob(os.path.join(img_path,class_,'*.png'))\n    for file in tqdm(files):\n        img=cv2.imread(file)\n        img=cv2.normalize(img,None,0,1,cv2.NORM_MINMAX,dtype=cv2.CV_32F)\n        img=cv2.resize(img,(150,150),cv2.INTER_AREA)\n        images.append(img)\n        if class_=='Uninfected':\n            target.append(0)\n        else:\n            target.append(1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target=np.reshape(target,(27558,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images,val_images,train_labels,val_labels=train_test_split(images,target,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Size of Train Data: {}'.format(len(train_images)))\nprint('Size of Val Data: {}'.format(len(val_images)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Placeholders\ninp=tf.placeholder(tf.float32,[None,150,150,3],name='Input_image')\nlabel=tf.placeholder(tf.float32,[None,1],'Input_label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Layer Functions\ndef Conv2D(input_,filters,kernel_size,strides,kernel_initializer='he_normal',padding='same'):\n    layer=tf.layers.conv2d(input_,filters=filters,kernel_size=kernel_size,strides=strides,kernel_initializer=kernel_initializer,\n                          padding=padding)\n    return layer\ndef LeakyReLU(x,t=0.2):\n    return tf.maximum(x,x*t)\ndef BatchNormalization(input_):\n    return tf.layers.batch_normalization(input_)\ndef MaxPooling2D(input_,pool_size,strides):\n    return tf.layers.max_pooling2d(input_,pool_size=pool_size,strides=strides)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Branch-1\nwith tf.name_scope('Branch-1'):\n    b1=Conv2D(inp,16,kernel_size=4,strides=1,kernel_initializer='he_uniform',padding='Valid')\n    b1=LeakyReLU(b1)\n    b1=Conv2D(b1,16,kernel_size=3,strides=1,kernel_initializer='he_uniform',padding='Valid')\n    b1=Conv2D(b1,16,kernel_size=3,strides=1,kernel_initializer='he_uniform',padding='Valid')\n    b1=BatchNormalization(b1)\n    b1_mp=MaxPooling2D(b1,pool_size=4,strides=4)\n\n#Branch-2\nwith tf.name_scope('Branch-2'):\n    b2=Conv2D(inp,32,kernel_size=4,strides=1,kernel_initializer='he_uniform',padding='Valid')\n    b2=LeakyReLU(b2)\n    b2=Conv2D(b2,32,kernel_size=3,strides=1,kernel_initializer='he_uniform',padding='Valid')\n    b2=Conv2D(b2,32,kernel_size=3,strides=1,kernel_initializer='he_uniform',padding='Valid')\n    b2_mp=MaxPooling2D(b2,pool_size=4,strides=4)\n    b2_mp=BatchNormalization(b2_mp)\n\n#Branch-3\nwith tf.name_scope('Branch-3'):\n    b3=Conv2D(inp,64,kernel_size=4,strides=1,kernel_initializer='he_uniform',padding='Valid')\n    b3=LeakyReLU(b3)\n    b3=Conv2D(b3,64,kernel_size=3,strides=1,kernel_initializer='he_uniform',padding='Valid')\n    b3=Conv2D(b3,64,kernel_size=3,strides=1,kernel_initializer='he_uniform',padding='Valid')\n    b3_mp=MaxPooling2D(b3,pool_size=4,strides=4)\n    b3_mp=BatchNormalization(b3_mp)\n\n\n#Branch_connections\nwith tf.name_scope('Branch-Conv'):\n    b1_con=Conv2D(b1_mp,32,kernel_size=3,strides=1,kernel_initializer='he_uniform',padding='Same')\n    b3=LeakyReLU(b3)\nwith tf.name_scope('Branch-Connect'):\n    con_1=tf.keras.layers.concatenate([b1_con,b2_mp])\n    con_2=tf.keras.layers.concatenate([con_1,b3_mp])\n\nwith tf.name_scope('Final_Block'):\n    layer_2=Conv2D(con_2,128,kernel_initializer='he_uniform',strides=1,padding='valid',kernel_size=1)\n    layer_2=LeakyReLU(layer_2)\n    layer_2_mp=MaxPooling2D(layer_2,pool_size=4,strides=4)\n\n    layer_3=Conv2D(layer_2,256,kernel_initializer='he_uniform',strides=1,padding='valid',kernel_size=3)\n    layer_3=LeakyReLU(layer_3)\n    layer_3_mp=MaxPooling2D(layer_3,pool_size=4,strides=4)\n    layer_3_mp=BatchNormalization(layer_3_mp)\n\n\n    layer_4=Conv2D(layer_3,512,kernel_initializer='he_uniform',strides=1,padding='valid',kernel_size=3)\n    layer_4=LeakyReLU(layer_4)\n    layer_4_mp=MaxPooling2D(layer_4,pool_size=4,strides=4)\n    layer_4_mp=BatchNormalization(layer_4_mp)\n\n    layer_5=Conv2D(layer_4,1024,kernel_initializer='he_uniform',strides=1,padding='valid',kernel_size=3)\n    layer_5=LeakyReLU(layer_5)\n    layer_5_mp=MaxPooling2D(layer_5,pool_size=6,strides=6)\n    layer_5_mp=BatchNormalization(layer_5_mp)\n\nwith tf.name_scope('Flatten'):\n    flat=tf.contrib.layers.flatten(layer_5_mp)\n    dense_1=tf.layers.dense(flat,2048,activation='relu',kernel_initializer='he_normal')\nwith tf.name_scope('Output_Units'):\n    out=tf.layers.dense(dense_1,1,activation=None)\n    prob=tf.nn.sigmoid(out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=label,logits=out))\ntrain_opt=tf.train.AdamOptimizer(0.0001).minimize(loss)\nacc = tf.metrics.auc(labels=label,predictions=prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=25\ntrain_batches=len(train_images)//batch_size\nval_batches=len(val_images)//batch_size\nepochs=30\ntrain_loss,val_loss=[],[]\ntrain_acc,val_acc=[],[]\nprint('Train Batches {}...Val Batches {}'.format(train_batches+1,val_batches+1))\nsaver=tf.train.Saver()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.Session() as sess:\n    writer = tf.summary.FileWriter(\"model_tensorboard\", sess.graph)\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    for epoch in range(epochs):\n        #Training\n        print('Epoch {} of {}'.format(epoch+1,epochs))\n        for batch in tqdm(range(train_batches+1)):\n            if batch==train_batches:\n                t_img=train_images[(batch)*batch_size:]\n                t_img=np.reshape(t_img,(len(t_img),150,150,3))\n                t_label=train_labels[(batch)*batch_size:]\n            else:\n                t_img=train_images[batch*batch_size:(batch+1)*batch_size]\n                t_img=np.reshape(t_img,(len(t_img),150,150,3))\n                t_label=train_labels[batch*batch_size:(batch+1)*batch_size]\n            t_loss,t_acc,_=sess.run([loss,acc,train_opt],feed_dict={inp:t_img,label:t_label})\n        train_loss.append(t_loss)\n        train_acc.append(t_acc)\n        #Validation\n        for batch in tqdm(range(val_batches+1)):\n            if batch==val_batches:\n                v_img=val_images[(batch)*batch_size:]\n                v_img=np.reshape(v_img,(len(v_img),150,150,3))\n                v_label=val_labels[(batch)*batch_size:]\n            else:\n                v_img=val_images[batch*batch_size:(batch+1)*batch_size]\n                v_img=np.reshape(v_img,(len(v_img),150,150,3))\n                v_label=val_labels[batch*batch_size:(batch+1)*batch_size]\n            v_loss,v_acc=sess.run([loss,acc],feed_dict={inp:v_img,label:v_label})\n        val_loss.append(v_loss)\n        val_acc.append(v_acc)\n        print('Training Loss: {}...Training Acc: {} '.format(t_loss,t_acc[0]))\n        print('Validation Loss: {}...Validation Acc: {}'.format(v_loss,v_acc[0]))\n        print('****************************************')    \n    writer.close()\n    saver.save(sess,'model_classifier.ckpt')\n    print('****************END********************')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Restore model and make predictions of validation set\npreds=[]\nwith tf.Session() as sess:\n    saver.restore(sess,'model_classifier.ckpt')\n    print('Classifier Restored')\n    for batch in tqdm(range(val_batches+1)):\n        if batch==val_batches:\n            v_img=val_images[(batch)*batch_size:]\n            v_img=np.reshape(v_img,(len(v_img),150,150,3))\n        else:\n            v_img=val_images[batch*batch_size:(batch+1)*batch_size]\n            v_img=np.reshape(v_img,(len(v_img),150,150,3))\n        p=sess.run(prob,feed_dict={inp:v_img})\n        preds.extend(p)\n    preds=['Parasitized' if i>=0.5 else 'Uninfected' for i in preds]\n    l=['Parasitized' if i>=0.5 else 'Uninfected' for i in val_labels]\n    #Plot predictions\n    random_images=np.reshape(val_images[20:24],(4,150,150,3))\n    random_preds=preds[20:24]\n    random_labels=l[20:24]\n    w,h=150,150\n    fig=plt.figure(figsize=(8,8))\n    columns=2\n    rows=2\n    for i in range(1,columns*rows+1):\n        fig.add_subplot(rows,columns,i)\n        img=io.imshow(random_images[i-1])\n        plt.title(['True:{}|| Predicted:{}'.format(random_labels[i-1],random_preds[i-1])])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix\ncm=confusion_matrix(y_true=l,y_pred=preds)\ncon_mat=pd.DataFrame(cm,index=['Uninfected','Parasitized'],columns=['Uninfected','Parasitized'])\nsns.heatmap(con_mat,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"con_mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss=train_loss\nval_loss=val_loss\nt_auc=[i[0] for i in train_acc]\nv_auc=[i[0] for i in val_acc]\nepochs=range(1,len(loss)+1)\nplt.plot(epochs,loss,'b',color='red',label='Training Loss')\nplt.plot(epochs,val_loss,'b',color='blue',label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.figure()\nplt.plot(epochs,t_auc,'b',color='red',label='Training AUC')\nplt.plot(epochs,v_auc,'b',color='blue',label='Validation AUC')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.figure()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}